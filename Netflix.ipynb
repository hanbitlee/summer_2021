{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Netflix",
      "provenance": [],
      "authorship_tag": "ABX9TyOvkJFK4ya1TUunD68Dao8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanbitlee/summer_2021/blob/main/Netflix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoKMCsx8PoMm",
        "outputId": "03f1f2a2-2244-4ff0-dcda-7d2252173a47"
      },
      "source": [
        "# utils\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/DSAIL_2021/dataset'\n",
        "\n",
        "def compute_sparse_correlation_matrix(A):\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    scaled_A = scaler.fit_transform(A)  # Assuming A is a CSR or CSC matrix\n",
        "    corr_matrix = (1/scaled_A.shape[0]) * (scaled_A.T @ scaled_A)\n",
        "    return corr_matrix\n",
        "\n",
        "def pre_processing(mat, mat_file):\n",
        "    # Create bu and bi indexes\n",
        "    # bi_index is a list with a size equal to the number of users\n",
        "    #    the jth element is a list storing the indexes of movies rated by user j\n",
        "    # bu_index is the same but storing the indexes of users whose rating is \n",
        "    #    available for a movie\n",
        "    # These indexes will help to the algorithms computation\n",
        "\n",
        "    shape = str(mat.shape[0])+\"_\"+str(mat.shape[1])\n",
        "    bu_index_file = mat_file+\"_bu_index_\"+shape+\".data\"\n",
        "    bi_index_file = mat_file+\"_bi_index_\"+shape+\".data\"\n",
        "\n",
        "    if not (os.path.isfile(bu_index_file) and os.path.isfile(bi_index_file)):\n",
        "        #mat = io.loadmat(mat_file)['X']\n",
        "        \"\"\"mat = mat[1:5000,1:5000]\n",
        "        mat = mat[mat.getnnz(1)>0][:,mat.getnnz(0)>0]\"\"\"\n",
        "\n",
        "        print(\"Pre-processing...\")\n",
        "        mat_nonzero = mat.nonzero()\n",
        "        \"\"\"cx = mat.tocoo()    \n",
        "        bi_index = [[]]*mat.shape[0]\n",
        "        bu_index = [[]]*mat.shape[1]\n",
        "        for i,j,v in zip(cx.row, cx.col, cx.data):\n",
        "          bi_index[i].append(j)\n",
        "          bu_index[j].append(i)\n",
        "        print(bi_index[0])\"\"\"\n",
        "\n",
        "        print(\"   make bi indexes...\")\n",
        "        bi_index = []\n",
        "        for k, g in groupby(zip(mat_nonzero[0], mat_nonzero[1]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bi_index.append(to_add)\n",
        "\n",
        "        print(\"   make bu indexes...\")\n",
        "        bu_index = []\n",
        "        indexes = np.argsort(mat_nonzero[1])\n",
        "        for k, g in groupby(zip(mat_nonzero[1][indexes], mat_nonzero[0][indexes]), itemgetter(0)):\n",
        "          to_add = list(map(lambda x:int(x[1]), list(g)))\n",
        "          bu_index.append(to_add)    \n",
        "\n",
        "        with open(bi_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bi_index, fp)\n",
        "        with open(bu_index_file, \"wb\") as fp:\n",
        "            pickle.dump(bu_index, fp)\n",
        "    else:\n",
        "        with open(bi_index_file, \"rb\") as fp:\n",
        "            bi_index = pickle.load(fp)\n",
        "        with open(bu_index_file, \"rb\") as fp:\n",
        "            bu_index = pickle.load(fp)\n",
        "\n",
        "    print(\"Pre-processing done.\")\n",
        "    return bu_index, bi_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdgISMz3fDLx",
        "outputId": "4213e100-cce2-4263-e69d-7f675622acc1"
      },
      "source": [
        "# rating complier\n",
        "import sys\n",
        "import os\n",
        "from scipy.sparse import dok_matrix, csr_matrix, lil_matrix\n",
        "from scipy import io\n",
        "import tarfile\n",
        "import numpy as np\n",
        "\n",
        "total_no_users = 2649429\n",
        "total_no_movies = 17770\n",
        "\n",
        "def process_content(content, D):\n",
        "    lines = content.split(\"\\n\")\n",
        "    id_movie = int(lines[0][:-1]) - 1\n",
        "    for i in range(1, len(lines)):\n",
        "        if lines[i] != '':\n",
        "            line = lines[i].split(\",\")\n",
        "            id_user = int(line[0]) - 1\n",
        "            rating = int(line[1])\n",
        "            D[id_user, id_movie] = rating\n",
        "    return D\n",
        "\n",
        "\n",
        "def rating_compiler(folder_name, out_path):\n",
        "    #D = dok_matrix((total_no_users, total_no_movies), dtype=np.uint8)\n",
        "    D = lil_matrix((total_no_users, total_no_movies), dtype=np.uint8)\n",
        "    res_listdir = os.listdir(folder_name)\n",
        "    number = len(res_listdir)\n",
        "    i = 0\n",
        "    for f in res_listdir:\n",
        "        if os.path.isfile(folder_name+f):\n",
        "            if i % 100 == 0:\n",
        "                print(i, \" / \", number)\n",
        "            myfile = open(folder_name+f)\n",
        "            content = myfile.read()\n",
        "            myfile.close()\n",
        "            D = process_content(content, D)\n",
        "        i += 1\n",
        "    D = csr_matrix(D)             \n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def rating_compiler2(tar_name, out_path):\n",
        "    #D = dok_matrix((total_no_users, total_no_movies), dtype=np.uint8)\n",
        "    D = lil_matrix((total_no_users, total_no_movies), dtype=np.uint8)\n",
        "    tar = tarfile.open(tar_name)\n",
        "    res_getmembers = tar.getmembers()\n",
        "    number = len(res_getmembers)\n",
        "    i = 0\n",
        "    for member in res_getmembers:\n",
        "        f = tar.extractfile(member)\n",
        "        if f is not None:    \n",
        "            if i % 100 == 0:\n",
        "                print(i, \" / \", number)        \n",
        "            content = f.read()\n",
        "            f.close()\n",
        "            D = process_content(content.decode(), D)\n",
        "        i += 1\n",
        "    tar.close()\n",
        "    D = csr_matrix(D, dtype=np.float64)\n",
        "    io.savemat(out_path, {'X' : D})\n",
        "\n",
        "\n",
        "def extract_T_and_R(D_file_name, file_name, out_T_path, out_R_path):\n",
        "    D = io.loadmat(D_file_name)['X']\n",
        "    myfile = open(file_name)\n",
        "    content = myfile.read()\n",
        "    myfile.close()\n",
        "    lines = content.split(\"\\n\")\n",
        "    users, movies = set(), set()\n",
        "    for line in lines:\n",
        "        if line != '':\n",
        "            line_split = line.split(\":\")\n",
        "            if len(line_split) == 2:\n",
        "                # Movie id\n",
        "                movies.add(int(line_split[0]) - 1)\n",
        "            else:\n",
        "                # User id\n",
        "                users.add(int(line_split[0]) - 1)\n",
        "    T = D[list(users),:]\n",
        "    T = T[:,list(movies)]    \n",
        "    io.savemat(out_T_path, {'X' : T})\n",
        "    \n",
        "    movies2 = set(range(total_no_movies))\n",
        "    movies2 = movies2.difference(movies)\n",
        "    users2 = set(range(total_no_users))\n",
        "    users2 = users2.difference(users)\n",
        "    \n",
        "    R = D[list(users2),:]\n",
        "    R = R[:,list(movies2)]\n",
        "    io.savemat(out_R_path, {'X' : R})\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    rating_compiler2(path+\"/training_set.tar\", path+\"/D.mat\")\n",
        "    #extract_T_and_R(path+\"/D.mat\", path+\"/download/qualifying.txt\", path+\"/T.mat\", path+\"/R.mat\")\n",
        "    extract_T_and_R(path+\"/D.mat\", path+\"/probe.txt\", path+\"/T.mat\", path+\"/R.mat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100  /  17771\n",
            "200  /  17771\n",
            "300  /  17771\n",
            "400  /  17771\n",
            "500  /  17771\n",
            "600  /  17771\n",
            "700  /  17771\n",
            "800  /  17771\n",
            "900  /  17771\n",
            "1000  /  17771\n",
            "1100  /  17771\n",
            "1200  /  17771\n",
            "1300  /  17771\n",
            "1400  /  17771\n",
            "1500  /  17771\n",
            "1600  /  17771\n",
            "1700  /  17771\n",
            "1800  /  17771\n",
            "1900  /  17771\n",
            "2000  /  17771\n",
            "2100  /  17771\n",
            "2200  /  17771\n",
            "2300  /  17771\n",
            "2400  /  17771\n",
            "2500  /  17771\n",
            "2600  /  17771\n",
            "2700  /  17771\n",
            "2800  /  17771\n",
            "2900  /  17771\n",
            "3000  /  17771\n",
            "3100  /  17771\n",
            "3200  /  17771\n",
            "3300  /  17771\n",
            "3400  /  17771\n",
            "3500  /  17771\n",
            "3600  /  17771\n",
            "3700  /  17771\n",
            "3800  /  17771\n",
            "3900  /  17771\n",
            "4000  /  17771\n",
            "4100  /  17771\n",
            "4200  /  17771\n",
            "4300  /  17771\n",
            "4400  /  17771\n",
            "4500  /  17771\n",
            "4600  /  17771\n",
            "4700  /  17771\n",
            "4800  /  17771\n",
            "4900  /  17771\n",
            "5000  /  17771\n",
            "5100  /  17771\n",
            "5200  /  17771\n",
            "5300  /  17771\n",
            "5400  /  17771\n",
            "5500  /  17771\n",
            "5600  /  17771\n",
            "5700  /  17771\n",
            "5800  /  17771\n",
            "5900  /  17771\n",
            "6000  /  17771\n",
            "6100  /  17771\n",
            "6200  /  17771\n",
            "6300  /  17771\n",
            "6400  /  17771\n",
            "6500  /  17771\n",
            "6600  /  17771\n",
            "6700  /  17771\n",
            "6800  /  17771\n",
            "6900  /  17771\n",
            "7000  /  17771\n",
            "7100  /  17771\n",
            "7200  /  17771\n",
            "7300  /  17771\n",
            "7400  /  17771\n",
            "7500  /  17771\n",
            "7600  /  17771\n",
            "7700  /  17771\n",
            "7800  /  17771\n",
            "7900  /  17771\n",
            "8000  /  17771\n",
            "8100  /  17771\n",
            "8200  /  17771\n",
            "8300  /  17771\n",
            "8400  /  17771\n",
            "8500  /  17771\n",
            "8600  /  17771\n",
            "8700  /  17771\n",
            "8800  /  17771\n",
            "8900  /  17771\n",
            "9000  /  17771\n",
            "9100  /  17771\n",
            "9200  /  17771\n",
            "9300  /  17771\n",
            "9400  /  17771\n",
            "9500  /  17771\n",
            "9600  /  17771\n",
            "9700  /  17771\n",
            "9800  /  17771\n",
            "9900  /  17771\n",
            "10000  /  17771\n",
            "10100  /  17771\n",
            "10200  /  17771\n",
            "10300  /  17771\n",
            "10400  /  17771\n",
            "10500  /  17771\n",
            "10600  /  17771\n",
            "10700  /  17771\n",
            "10800  /  17771\n",
            "10900  /  17771\n",
            "11000  /  17771\n",
            "11100  /  17771\n",
            "11200  /  17771\n",
            "11300  /  17771\n",
            "11400  /  17771\n",
            "11500  /  17771\n",
            "11600  /  17771\n",
            "11700  /  17771\n",
            "11800  /  17771\n",
            "11900  /  17771\n",
            "12000  /  17771\n",
            "12100  /  17771\n",
            "12200  /  17771\n",
            "12300  /  17771\n",
            "12400  /  17771\n",
            "12500  /  17771\n",
            "12600  /  17771\n",
            "12700  /  17771\n",
            "12800  /  17771\n",
            "12900  /  17771\n",
            "13000  /  17771\n",
            "13100  /  17771\n",
            "13200  /  17771\n",
            "13300  /  17771\n",
            "13400  /  17771\n",
            "13500  /  17771\n",
            "13600  /  17771\n",
            "13700  /  17771\n",
            "13800  /  17771\n",
            "13900  /  17771\n",
            "14000  /  17771\n",
            "14100  /  17771\n",
            "14200  /  17771\n",
            "14300  /  17771\n",
            "14400  /  17771\n",
            "14500  /  17771\n",
            "14600  /  17771\n",
            "14700  /  17771\n",
            "14800  /  17771\n",
            "14900  /  17771\n",
            "15000  /  17771\n",
            "15100  /  17771\n",
            "15200  /  17771\n",
            "15300  /  17771\n",
            "15400  /  17771\n",
            "15500  /  17771\n",
            "15600  /  17771\n",
            "15700  /  17771\n",
            "15800  /  17771\n",
            "15900  /  17771\n",
            "16000  /  17771\n",
            "16100  /  17771\n",
            "16200  /  17771\n",
            "16300  /  17771\n",
            "16400  /  17771\n",
            "16500  /  17771\n",
            "16600  /  17771\n",
            "16700  /  17771\n",
            "16800  /  17771\n",
            "16900  /  17771\n",
            "17000  /  17771\n",
            "17100  /  17771\n",
            "17200  /  17771\n",
            "17300  /  17771\n",
            "17400  /  17771\n",
            "17500  /  17771\n",
            "17600  /  17771\n",
            "17700  /  17771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHetmzUngDNn",
        "outputId": "15d821dc-d074-4d84-acdb-40e3e8545718"
      },
      "source": [
        "# baseline estimator\n",
        "from scipy import io, sparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, l_reg=0.02):\n",
        "  loss = 0\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1)).T.ravel()\n",
        "  bu_rep = np.repeat(bu.ravel(), no_users_entries)\n",
        "\n",
        "  no_movies_entries = np.array((mat != 0).sum(0)).ravel()\n",
        "  bi_rep = np.repeat(bi.ravel(), no_movies_entries)\n",
        "\n",
        "  temp_mat = sparse.csc_matrix(mat).copy()\n",
        "  temp_mat.data[:] -= bi_rep\n",
        "  temp_mat.data[:] -= mu\n",
        "  temp_mat = sparse.coo_matrix(temp_mat)\n",
        "  temp_mat = sparse.csr_matrix(temp_mat)\n",
        "  temp_mat.data[:] -= bu_rep\n",
        "\n",
        "  loss = (temp_mat.data[:] ** 2).sum()\n",
        "\n",
        "  loss_reg = l_reg * ((bu**2).sum() + (bi**2).sum())  \n",
        "  #loss += loss_reg\n",
        "\n",
        "  return loss, loss+loss_reg\n",
        "\n",
        "def baseline_estimator(mat, mat_file, l_reg=0.02, learning_rate=0.0000025):\n",
        "  # subsample the matrix to make computation faster\n",
        "  mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "  mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "  print(mat.shape)\n",
        "  no_users = mat.shape[0]\n",
        "  no_movies = mat.shape[1]\n",
        "  \n",
        "  bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "\n",
        "  bu = np.random.rand(no_users,1)  * 2 - 1\n",
        "  bi = np.random.rand(1,no_movies) * 2 - 1\n",
        "  #bu = np.zeros((no_users,1))\n",
        "  #bi = np.zeros((1,no_movies))  \n",
        "\n",
        "  mu = mat.data[:].mean()\n",
        "  mat_sum1 = mat.sum(1)\n",
        "  mat_sum0 = mat.sum(0)\n",
        "  n = mat.data[:].shape[0]\n",
        "\n",
        "  no_users_entries = np.array((mat != 0).sum(1))\n",
        "  no_movies_entries = np.array((mat != 0).sum(0))\n",
        "\n",
        "  # Train\n",
        "  print(\"Train...\")\n",
        "  n_iter = 500\n",
        "  for it in range(n_iter):\n",
        "\n",
        "    #bi_sum = bi[bi_index].sum(1).reshape((no_users,1))\n",
        "    #bu_sum = bu.ravel()[bu_index].sum(0).reshape((1,no_movies)) \n",
        "\n",
        "    bi_sum = np.array(list(map(lambda x:bi.ravel()[x].sum(), bi_index))).reshape((no_users,1))\n",
        "    bu_sum = np.array(list(map(lambda x:bu.ravel()[x].sum(), bu_index))).reshape((1,no_movies))    \n",
        "\n",
        "    # Vectorized operations\n",
        "    bu_gradient = - 2.0 * (mat_sum1 - no_users_entries  * mu - no_users_entries  * bu - bi_sum) + 2.0 * l_reg * bu\n",
        "    bu -= learning_rate * bu_gradient \n",
        "\n",
        "    bi_gradient = - 2.0 * (mat_sum0 - no_movies_entries * mu - no_movies_entries * bi - bu_sum) + 2.0 * l_reg * bi\n",
        "    bi -= learning_rate * bi_gradient \n",
        " \n",
        "    if it % 10 == 0:\n",
        "      print(\"compute loss...\")\n",
        "      print(compute_loss(mat, mu, bu, bi, l_reg=l_reg))\n",
        "\n",
        "  return bu, bi\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']    \n",
        "    baseline_estimator(mat, mat_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1542, 111)\n",
            "Pre-processing done.\n",
            "Train...\n",
            "compute loss...\n",
            "(6666.796287039252, 6677.666783356851)\n",
            "compute loss...\n",
            "(6607.588102143827, 6618.452696332805)\n",
            "compute loss...\n",
            "(6551.9686761138055, 6562.82749041646)\n",
            "compute loss...\n",
            "(6499.651548675032, 6510.504697915567)\n",
            "compute loss...\n",
            "(6450.374523800499, 6461.222115958567)\n",
            "compute loss...\n",
            "(6403.897584972293, 6414.7397217089565)\n",
            "compute loss...\n",
            "(6360.000990214845, 6370.837767354987)\n",
            "compute loss...\n",
            "(6318.4835313823605, 6329.315039357244)\n",
            "compute loss...\n",
            "(6279.160943522958, 6289.987267776323)\n",
            "compute loss...\n",
            "(6241.864451366178, 6252.685672726954)\n",
            "compute loss...\n",
            "(6206.439441098826, 6217.2556361232955)\n",
            "compute loss...\n",
            "(6172.744246616001, 6183.555487901992)\n",
            "compute loss...\n",
            "(6140.649040367625, 6151.455396843098)\n",
            "compute loss...\n",
            "(6110.034819773864, 6120.836356962055)\n",
            "compute loss...\n",
            "(6080.79248096206, 6091.589261225152)\n",
            "compute loss...\n",
            "(6052.821972289868, 6063.614055052999)\n",
            "compute loss...\n",
            "(6026.031520769831, 6036.818962727097)\n",
            "compute loss...\n",
            "(6000.336925104986, 6011.119780408942)\n",
            "compute loss...\n",
            "(5975.660909588167, 5986.4392300242125)\n",
            "compute loss...\n",
            "(5951.932533613823, 5962.706368760744)\n",
            "compute loss...\n",
            "(5929.08665200452, 5939.856049382331)\n",
            "compute loss...\n",
            "(5907.063421768437, 5917.828426974588)\n",
            "compute loss...\n",
            "(5885.8078512826405, 5896.568508117565)\n",
            "compute loss...\n",
            "(5865.26938824262, 5876.0257388254995)\n",
            "compute loss...\n",
            "(5845.401543034522, 5856.15362791008)\n",
            "compute loss...\n",
            "(5826.161544475066, 5836.909402712141)\n",
            "compute loss...\n",
            "(5807.5100251279155, 5818.2536944104795)\n",
            "compute loss...\n",
            "(5789.410733646136, 5800.150250357394)\n",
            "compute loss...\n",
            "(5771.830271810542, 5782.565671110672)\n",
            "compute loss...\n",
            "(5754.737854134866, 5765.469170032923)\n",
            "compute loss...\n",
            "(5738.105088092416, 5748.832353512878)\n",
            "compute loss...\n",
            "(5721.905773186798, 5732.629020031191)\n",
            "compute loss...\n",
            "(5706.1157172427065, 5716.834976446707)\n",
            "compute loss...\n",
            "(5690.712568432888, 5701.427870019265)\n",
            "compute loss...\n",
            "(5675.67566168547, 5686.387034813217)\n",
            "compute loss...\n",
            "(5660.9858782328765, 5671.693351242817)\n",
            "compute loss...\n",
            "(5646.625517170378, 5657.32911762755)\n",
            "compute loss...\n",
            "(5632.578177990066, 5643.277932723131)\n",
            "compute loss...\n",
            "(5618.828653145229, 5629.524588283138)\n",
            "compute loss...\n",
            "(5605.362829781681, 5616.054970787827)\n",
            "compute loss...\n",
            "(5592.167599847067, 5602.855971551111)\n",
            "compute loss...\n",
            "(5579.230777857213, 5589.915404484772)\n",
            "compute loss...\n",
            "(5566.541025660841, 5577.221930861205)\n",
            "compute loss...\n",
            "(5554.087783600726, 5564.764990472751)\n",
            "compute loss...\n",
            "(5541.861207521313, 5552.5347386376425)\n",
            "compute loss...\n",
            "(5529.852111120264, 5540.521988549991)\n",
            "compute loss...\n",
            "(5518.051913184694, 5528.718158514597)\n",
            "compute loss...\n",
            "(5506.452589292522, 5517.1152236469825)\n",
            "compute loss...\n",
            "(5495.0466275954805, 5505.705671655167)\n",
            "compute loss...\n",
            "(5483.8269883334215, 5494.482462352848)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8NsZsh5xehN",
        "outputId": "5b1206d6-68fe-48a1-94cb-1d94171c4238"
      },
      "source": [
        "# correlation based neighborhood model\n",
        "import numpy as np\n",
        "from scipy import io, sparse\n",
        "from math import sqrt\n",
        "from time import time\n",
        "\n",
        "#################################################\n",
        "# Non-vectorized way (iterate through each r_ui)\n",
        "#################################################\n",
        "def predict_r_ui(mat, u, i, mu, S, Sk_iu, baseline_bu, baseline_bi):\n",
        "  bui = mu + baseline_bu[u] + baseline_bi[0, i]\n",
        "  buj = mu + baseline_bu[u] + baseline_bi[0, Sk_iu]\n",
        "  return bui + 1 / S[i, Sk_iu].sum() * (S[i, Sk_iu].toarray().ravel() * (mat[u, Sk_iu].toarray().ravel() - buj)).sum()\n",
        "\n",
        "def correlation_based_neighbourhood_model(mat, mat_file, l_reg2=100.0, k=250):\n",
        "    # subsample the matrix to make computation faster\n",
        "    mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for test\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    #bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    # Computation\n",
        "    print(\"Computation...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()\n",
        "    r_ui_mat = []\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        Sk_iu = np.flip(np.argsort(S[i,].toarray()))[:k].ravel()\n",
        "        r_ui = predict_r_ui(mat, u, i, mu, S, Sk_iu, baseline_bu, baseline_bi)\n",
        "        r_ui_mat.append((u, i, r_ui[0]))\n",
        "\n",
        "    data = list(map(lambda x: x[2], r_ui_mat))\n",
        "    col = list(map(lambda x: x[1], r_ui_mat))\n",
        "    row = list(map(lambda x: x[0], r_ui_mat))\n",
        "    r_ui_pred = sparse.csr_matrix((data, (row, col)), shape=mat.shape)\n",
        "\n",
        "    print((mat - r_ui_pred).sum())\n",
        "\n",
        "    return r_ui_pred\n",
        "\n",
        "#################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    correlation_based_neighbourhood_model(mat, mat_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1542, 111)\n",
            "Computation...\n",
            "9917.756249908556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iescXSgYlqwm",
        "outputId": "2013d1d2-80f1-4eb4-f5e0-96a27fa6a575"
      },
      "source": [
        "# correlation based implicit neighborhood model\n",
        "def predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi):\n",
        "    buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "    Rk_iu_sum = np.multiply((mat[u, Rk_iu] - buj), wij[i][Rk_iu]).sum()\n",
        "    Nk_iu_sum = cij[i][Rk_iu].sum()\n",
        "    return mu + bu[u] + bi[0, i] + Rk_iu_sum / sqrt(len(Rk_iu)) + Nk_iu_sum / sqrt(len(Nk_iu))\n",
        "\n",
        "def compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi):\n",
        "    return mat[u, i] - predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi)\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, l_reg=0.002):\n",
        "    loss = 0\n",
        "    loss_reg = 0\n",
        "    cx = mat.tocoo()        \n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        r_ui_pred = predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi)\n",
        "        Rk_iu_sum = (wij[i][Rk_iu] ** 2).sum()\n",
        "        Nk_iu_sum = (cij[i][Rk_iu] ** 2).sum()\n",
        "        loss += (mat[u, i] - r_ui_pred) ** 2 \n",
        "        loss_reg += l_reg * ((bu ** 2).sum() + (bi ** 2).sum() + Rk_iu_sum + Nk_iu_sum) \n",
        "\n",
        "    return loss, loss+loss_reg\n",
        "\n",
        "def correlation_based_implicit_neighbourhood_model(mat, mat_file, l_reg=0.002, gamma=0.005, l_reg2=100.0, k=250):\n",
        "    # subsample the matrix to make computation faster\n",
        "    mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for test\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    # Init parameters\n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    wij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    cij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()        \n",
        "    for it in range(n_iter):\n",
        "        t0 = time()\n",
        "        for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "            #Rk_iu = Nk_iu = bi_index[u]\n",
        "            Rk_iu = Nk_iu = np.flip(np.argsort(S[i,].toarray()))[:k].ravel()\n",
        "            e_ui = compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi)\n",
        "\n",
        "            bu[u] += gamma * (e_ui - l_reg * bu[u])\n",
        "            bi[0, i] += gamma * (e_ui - l_reg * bi[0, i])\n",
        "\n",
        "            buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "            wij[i][Rk_iu] += gamma * ( 1 / sqrt(len(Rk_iu)) * e_ui * (mat[u, Rk_iu].toarray().ravel() - buj) - l_reg * wij[i][Rk_iu] )\n",
        "            cij[i][Nk_iu] += gamma * ( 1 / sqrt(len(Nk_iu)) * e_ui - l_reg * cij[i][Nk_iu] )\n",
        "        gamma *= 0.99\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          t1 = time()\n",
        "          print(it, \"\\ \", n_iter, \"(%.2g sec)\" % (t1 - t0))\n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, l_reg=l_reg))\n",
        "\n",
        "    return bu, bi, wij, cij\n",
        "#################################################\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Vectorized way (in work)\n",
        "# (Actually this version is faster but updates e_ui\n",
        "# less frequently making it less accurate for the\n",
        "# gradient descent)\n",
        "#################################################\n",
        "def compute_e_vectorized(mat, mu, bu, bi, Rk, wij, Nk, cij, baseline_bu, baseline_bi):\n",
        "    # Rk and Nk are list of tuple (u, i, Rk_iu/Nk_iu)\n",
        "\n",
        "    no_users_entries = np.array((mat != 0).sum(1)).T.ravel()\n",
        "    bu_rep = np.repeat(bu.ravel(), no_users_entries)\n",
        "\n",
        "    no_movies_entries = np.array((mat != 0).sum(0)).ravel()\n",
        "    bi_rep = np.repeat(bi.ravel(), no_movies_entries)\n",
        "\n",
        "    temp_mat = sparse.csc_matrix(mat).copy()\n",
        "    temp_mat.data[:] -= mu\n",
        "    temp_mat.data[:] -= bi_rep\n",
        "    Rk_sum = np.array(list(map(lambda x : ( (mat[x[0], x[2]].toarray().ravel() \\\n",
        "                                           - (mu + baseline_bu[x[0]] + baseline_bi[0, x[2]])) \\\n",
        "                                           * wij[x[1]][x[2]] ).sum() / sqrt(len(x[2])), Rk)))\n",
        "    temp_mat.data[:] -= Rk_sum\n",
        "    Nk_sum = np.array(list(map(lambda x : cij[x[1]][x[2]].sum() / sqrt(len(x[2])), Nk)))\n",
        "    temp_mat.data[:] -= Nk_sum\n",
        "    temp_mat = sparse.coo_matrix(temp_mat)\n",
        "    temp_mat = sparse.csr_matrix(temp_mat)\n",
        "    temp_mat.data[:] -= bu_rep\n",
        "\n",
        "    return temp_mat\n",
        "\n",
        "def compute_loss_vectorized(mat, mu, bu, bi, Rk, wij, Nk, cij, baseline_bu, baseline_bi, l_reg=0.002):\n",
        "    no_nonzero_element = np.array((mat != 0).sum())\n",
        "    loss = (compute_e_vectorized(mat, mu, bu, bi, Rk, wij, Nk, cij, baseline_bu, baseline_bi).data[:] ** 2).sum()\n",
        "    loss_reg = l_reg * np.array(list(map(lambda x : (cij[x[1]][x[2]] ** 2).sum(), Nk))).sum()\n",
        "    loss_reg += l_reg * np.array(list(map(lambda x : (wij[x[1]][x[2]] ** 2).sum(), Rk))).sum()\n",
        "    loss_reg += no_nonzero_element * l_reg * (bu ** 2).sum()\n",
        "    loss_reg += no_nonzero_element * l_reg * (bi ** 2).sum()\n",
        "\n",
        "    return loss, loss+loss_reg\n",
        "\n",
        "def correlation_based_implicit_neighbourhood_model_vectorized(mat, mat_file, l_reg=0.002, gamma=0.005, l_reg2=100.0, k=250):\n",
        "    gamma /= 100\n",
        "\n",
        "    # subsample the matrix to make computation faster\n",
        "    mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "    no_users_entries = np.array((mat != 0).sum(1))\n",
        "    no_movies_entries = np.array((mat != 0).sum(0))    \n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for testing\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    wij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    cij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    Rk = []\n",
        "    cx = mat.tocoo()\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        Rk.append((u, i, np.flip(np.argsort(S[i,].toarray()))[:k].ravel()))\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    for it in range(n_iter):\n",
        "        t0 = time() \n",
        "\n",
        "        e = compute_e_vectorized(mat, mu, bu, bi, Rk, wij, Rk, cij, baseline_bu, baseline_bi)\n",
        "        # Vectorized operations\n",
        "        bu += gamma * (e.sum(1) - no_users_entries * l_reg * bu)\n",
        "        bi += gamma * (e.sum(0) - no_movies_entries * l_reg * bi)\n",
        "\n",
        "        # TODO: vectorize the following\n",
        "        for u, i, Rk_iu in Rk:\n",
        "            Nk_iu = Rk_iu\n",
        "            e_ui = e[u, i]\n",
        "            buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "            wij[i][Rk_iu] += gamma * ( 1 / sqrt(len(Rk_iu)) * e_ui * (mat[u, Rk_iu].toarray().ravel() - buj) - l_reg * wij[i][Rk_iu] )\n",
        "            cij[i][Nk_iu] += gamma * ( 1 / sqrt(len(Nk_iu)) * e_ui - l_reg * cij[i][Nk_iu] )\n",
        "        gamma *= 0.99\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          t1 = time()\n",
        "          print(it, \"\\ \", n_iter, \"(%.2g sec)\" % (t1 - t0))       \n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss_vectorized(mat, mu, bu, bi, Rk, wij, Rk, cij, baseline_bu, baseline_bi, l_reg=l_reg))  \n",
        "\n",
        "    return bu, bi, wij, cij\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    correlation_based_implicit_neighbourhood_model(mat, mat_file)\n",
        "    #correlation_based_implicit_neighbourhood_model_vectorized(mat, mat_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1542, 111)\n",
            "Pre-processing done.\n",
            "Train...\n",
            "0 \\  200 (3.9 sec)\n",
            "compute loss...\n",
            "(array([5561.89502716]), array([9802.98569185]))\n",
            "10 \\  200 (3.8 sec)\n",
            "compute loss...\n",
            "(array([3176.34320461]), array([6858.65366155]))\n",
            "20 \\  200 (3.9 sec)\n",
            "compute loss...\n",
            "(array([2500.27756981]), array([5878.71136589]))\n",
            "30 \\  200 (3.9 sec)\n",
            "compute loss...\n",
            "(array([2102.48899182]), array([5289.68942295]))\n",
            "40 \\  200 (4 sec)\n",
            "compute loss...\n",
            "(array([1838.40702033]), array([4895.72572093]))\n",
            "50 \\  200 (4.2 sec)\n",
            "compute loss...\n",
            "(array([1650.61035182]), array([4615.24764106]))\n",
            "60 \\  200 (4.1 sec)\n",
            "compute loss...\n",
            "(array([1510.65221606]), array([4406.84280321]))\n",
            "70 \\  200 (4.2 sec)\n",
            "compute loss...\n",
            "(array([1402.72417312]), array([4247.0693262]))\n",
            "80 \\  200 (4.3 sec)\n",
            "compute loss...\n",
            "(array([1317.31122879]), array([4121.60577932]))\n",
            "90 \\  200 (4.1 sec)\n",
            "compute loss...\n",
            "(array([1248.33135135]), array([4021.18973611]))\n",
            "100 \\  200 (4 sec)\n",
            "compute loss...\n",
            "(array([1191.7095]), array([3939.56146427]))\n",
            "110 \\  200 (4.1 sec)\n",
            "compute loss...\n",
            "(array([1144.61000808]), array([3872.34044137]))\n",
            "120 \\  200 (4.1 sec)\n",
            "compute loss...\n",
            "(array([1104.99731833]), array([3816.37312695]))\n",
            "130 \\  200 (4.4 sec)\n",
            "compute loss...\n",
            "(array([1071.37180767]), array([3769.3352962]))\n",
            "140 \\  200 (4.2 sec)\n",
            "compute loss...\n",
            "(array([1042.60419214]), array([3729.47961314]))\n",
            "150 \\  200 (4.3 sec)\n",
            "compute loss...\n",
            "(array([1017.82796697]), array([3695.46987441]))\n",
            "160 \\  200 (4.1 sec)\n",
            "compute loss...\n",
            "(array([996.3673258]), array([3666.26895317]))\n",
            "170 \\  200 (4 sec)\n",
            "compute loss...\n",
            "(array([977.6875112]), array([3641.06111821]))\n",
            "180 \\  200 (4.5 sec)\n",
            "compute loss...\n",
            "(array([961.35979962]), array([3619.19701233]))\n",
            "190 \\  200 (4.3 sec)\n",
            "compute loss...\n",
            "(array([947.03632024]), array([3600.15397067]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfAL4QQal3Ky",
        "outputId": "8828da82-017b-415e-f9ec-1852c51dd67c"
      },
      "source": [
        "# SVD\n",
        "\n",
        "def predict_r_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj):\n",
        "    N_u_sum = yj[N_u].sum(0)\n",
        "    return mu + bu[u] + bi[0, i] + np.dot(qi[i], (pu[u] + N_u_sum / sqrt(len(N_u))))\n",
        "\n",
        "def compute_e_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj):\n",
        "    return mat[u, i] - predict_r_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj)\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, qi, pu, N_u, yj, l_reg6=0.005, l_reg7=0.015):\n",
        "    loss = 0\n",
        "    loss_reg = 0\n",
        "    cx = mat.tocoo()\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        r_ui_pred = predict_r_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj)\n",
        "        loss += (mat[u, i] - r_ui_pred) ** 2 \n",
        "        loss_reg += l_reg6 * ((bu ** 2).sum() + (bi ** 2).sum())\n",
        "        loss_reg += l_reg7 * ((qi[i]**2).sum() + (pu[u]**2).sum() + (yj[N_u]**2).sum())\n",
        "\n",
        "    return loss, loss+loss_reg\n",
        "\n",
        "def svd_more_more(mat, mat_file, gamma1=0.007, gamma2=0.007, gamma3=0.001, l_reg2=100, l_reg6=0.005, l_reg7=0.015, f=50):\n",
        "    # subsample the matrix to make computation faster\n",
        "    mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    # Init parameters\n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    qi = np.random.rand(no_movies, f) * 2 - 1\n",
        "    pu = np.random.rand(no_users, f) * 2 - 1\n",
        "    yj = np.random.rand(no_movies, f) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()\n",
        "    for it in range(n_iter):\n",
        "        for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "            N_u = bi_index[u]\n",
        "            e_ui = compute_e_ui(mat, u, i, mu, bu, bi, qi, pu, N_u, yj)\n",
        "\n",
        "            bu[u] += gamma1 * (e_ui - l_reg6 * bu[u])\n",
        "            bi[0, i] += gamma1 * (e_ui - l_reg6 * bi[0, i])\n",
        "            qi[i] += gamma2 * (e_ui * (pu[u] + 1 / sqrt(len(N_u)) * yj[N_u].sum(0)) - l_reg7 * qi[i])\n",
        "            pu[u] += gamma2 * (e_ui * qi[i] - l_reg7 * pu[u])\n",
        "            yj[N_u] += gamma2 * (e_ui * 1/ sqrt(len(N_u)) * qi[i] - l_reg7 * yj[N_u])\n",
        "        gamma1 *= 0.9\n",
        "        gamma2 *= 0.9\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          print(it, \"\\ \", n_iter)         \n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss(mat, mu, bu, bi, qi, pu, N_u, yj, l_reg6=l_reg6, l_reg7=l_reg7))\n",
        "    \n",
        "    return bu, bi, qi, pu, yj\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    svd_more_more(mat, mat_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1542, 111)\n",
            "Pre-processing done.\n",
            "Train...\n",
            "0 \\  200\n",
            "compute loss...\n",
            "(array([14082.22893435]), array([25244.03157806]))\n",
            "10 \\  200\n",
            "compute loss...\n",
            "(array([7061.77539024]), array([17136.30901515]))\n",
            "20 \\  200\n",
            "compute loss...\n",
            "(array([6447.19248705]), array([16297.40936531]))\n",
            "30 \\  200\n",
            "compute loss...\n",
            "(array([6272.80440737]), array([16053.03032861]))\n",
            "40 \\  200\n",
            "compute loss...\n",
            "(array([6215.52361908]), array([15972.22931425]))\n",
            "50 \\  200\n",
            "compute loss...\n",
            "(array([6195.93842191]), array([15944.54672393]))\n",
            "60 \\  200\n",
            "compute loss...\n",
            "(array([6189.15501587]), array([15934.9523719]))\n",
            "70 \\  200\n",
            "compute loss...\n",
            "(array([6186.79525984]), array([15931.61400561]))\n",
            "80 \\  200\n",
            "compute loss...\n",
            "(array([6185.9731263]), array([15930.4508346]))\n",
            "90 \\  200\n",
            "compute loss...\n",
            "(array([6185.6865465]), array([15930.04536459]))\n",
            "100 \\  200\n",
            "compute loss...\n",
            "(array([6185.58663207]), array([15929.90399842]))\n",
            "110 \\  200\n",
            "compute loss...\n",
            "(array([6185.55179526]), array([15929.8547086]))\n",
            "120 \\  200\n",
            "compute loss...\n",
            "(array([6185.53964855]), array([15929.83752248]))\n",
            "130 \\  200\n",
            "compute loss...\n",
            "(array([6185.53541328]), array([15929.83153008]))\n",
            "140 \\  200\n",
            "compute loss...\n",
            "(array([6185.53393653]), array([15929.82944066]))\n",
            "150 \\  200\n",
            "compute loss...\n",
            "(array([6185.53342162]), array([15929.82871213]))\n",
            "160 \\  200\n",
            "compute loss...\n",
            "(array([6185.53324208]), array([15929.8284581]))\n",
            "170 \\  200\n",
            "compute loss...\n",
            "(array([6185.53317948]), array([15929.82836953]))\n",
            "180 \\  200\n",
            "compute loss...\n",
            "(array([6185.53315765]), array([15929.82833865]))\n",
            "190 \\  200\n",
            "compute loss...\n",
            "(array([6185.53315004]), array([15929.82832788]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuWm2Hrt8ivH",
        "outputId": "ab2c0e6e-1a82-493d-d538-8fd9fa9dd451"
      },
      "source": [
        "# integrated model\n",
        "from math import sqrt, isnan\n",
        "\n",
        "# Through all this code Rk_iu and Nk_iu are the same since implicit matrix is\n",
        "#    made from the rating matrix without additional information.\n",
        "\n",
        "#################################################\n",
        "# Non-vectorized way\n",
        "#################################################\n",
        "def predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj):\n",
        "    buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "    Rk_iu_sum = np.multiply((mat[u, Rk_iu] - buj), wij[i][Rk_iu]).sum()\n",
        "    Nk_iu_sum = cij[i][Rk_iu].sum()\n",
        "    N_u_sum = yj[N_u].sum(0)\n",
        "    return mu + bu[u] + bi[0, i] + np.dot(qi[i], (pu[u] + N_u_sum / sqrt(len(N_u)))) + Rk_iu_sum / sqrt(len(Rk_iu)) + Nk_iu_sum / sqrt(len(Nk_iu))\n",
        "\n",
        "def compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj):\n",
        "    return mat[u, i] - predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj)\n",
        "\n",
        "def compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj, l_reg6=0.005, l_reg7=0.015, l_reg8=0.015):\n",
        "    loss = 0\n",
        "    loss_reg = 0\n",
        "    cx = mat.tocoo()\n",
        "    for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "        r_ui_pred = predict_r_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj)\n",
        "        Rk_iu_sum = (wij[i][Rk_iu] ** 2).sum()\n",
        "        Nk_iu_sum = (cij[i][Rk_iu] ** 2).sum()\n",
        "        loss += (mat[u, i] - r_ui_pred) ** 2\n",
        "        loss_reg += l_reg6 * ((bu ** 2).sum() + (bi ** 2).sum())\n",
        "        loss_reg += l_reg8 * (Rk_iu_sum + Nk_iu_sum) \n",
        "        loss_reg += l_reg7 * ((qi[i]**2).sum() + (pu[u]**2).sum() + (yj[N_u]**2).sum())\n",
        "\n",
        "    return loss, loss+loss_reg\n",
        "\n",
        "def integrated_model(mat, mat_file, gamma1=0.007, gamma2=0.007, gamma3=0.001, l_reg2=100, l_reg6=0.005, l_reg7=0.015, l_reg8=0.015, k=300, f=50):\n",
        "    # subsample the matrix to make computation faster\n",
        "    mat = mat[0:mat.shape[0]//128, 0:mat.shape[1]//128]\n",
        "    mat = mat[mat.getnnz(1)>0][:, mat.getnnz(0)>0]\n",
        "\n",
        "    print(mat.shape)\n",
        "    no_users = mat.shape[0]\n",
        "    no_movies = mat.shape[1]\n",
        "\n",
        "    #baseline_bu, baseline_bi = baseline_estimator(mat)\n",
        "    # We should call baseline_estimator but we can init at random for test\n",
        "    baseline_bu, baseline_bi = np.random.rand(no_users, 1)  * 2 - 1, np.random.rand(1, no_movies) * 2 - 1    \n",
        "\n",
        "    bu_index, bi_index = pre_processing(mat, mat_file)\n",
        "    \n",
        "    # Init parameters\n",
        "    bu = np.random.rand(no_users, 1)  * 2 - 1\n",
        "    bi = np.random.rand(1, no_movies) * 2 - 1\n",
        "    wij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    cij = np.random.rand(no_movies, no_movies) * 2 - 1\n",
        "    qi = np.random.rand(no_movies, f) * 2 - 1\n",
        "    pu = np.random.rand(no_users, f) * 2 - 1\n",
        "    yj = np.random.rand(no_movies, f) * 2 - 1\n",
        "\n",
        "    mu = mat.data[:].mean()\n",
        "    N = sparse.csr_matrix(mat).copy()\n",
        "    N.data[:] = 1\n",
        "    S = sparse.csr_matrix.dot(N.T, N)\n",
        "    S.data[:] = S.data[:] / (S.data[:] + l_reg2)\n",
        "    S = S * compute_sparse_correlation_matrix(mat)\n",
        "\n",
        "    # Train\n",
        "    print(\"Train...\")\n",
        "    n_iter = 200\n",
        "    cx = mat.tocoo()\n",
        "    for it in range(n_iter):\n",
        "        for u,i,v in zip(cx.row, cx.col, cx.data):\n",
        "            #Rk_iu = Nk_iu = bi_index[u]\n",
        "            N_u = bi_index[u]\n",
        "            Rk_iu = Nk_iu = np.flip(np.argsort(S[i,].toarray()))[:k].ravel()\n",
        "            e_ui = compute_e_ui(mat, u, i, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj)\n",
        "\n",
        "            bu[u] += gamma1 * (e_ui - l_reg6 * bu[u])\n",
        "            bi[0, i] += gamma1 * (e_ui - l_reg6 * bi[0, i])\n",
        "            qi[i] += gamma2 * (e_ui * (pu[u] + 1 / sqrt(len(N_u)) * yj[N_u].sum(0)) - l_reg7 * qi[i])\n",
        "            pu[u] += gamma2 * (e_ui * qi[i] - l_reg7 * pu[u])\n",
        "            yj[N_u] += gamma2 * (e_ui * 1/ sqrt(len(N_u)) * qi[i] - l_reg7 * yj[N_u])\n",
        "            buj = mu + baseline_bu[u] + baseline_bi[0, Rk_iu]\n",
        "            wij[i][Rk_iu] += gamma3 * ( 1 / sqrt(len(Rk_iu)) * e_ui * (mat[u, Rk_iu].toarray().ravel() - buj) - l_reg8 * wij[i][Rk_iu] )\n",
        "            cij[i][Nk_iu] += gamma3 * ( 1 / sqrt(len(Nk_iu)) * e_ui - l_reg8 * cij[i][Nk_iu] )                \n",
        "        gamma1 *= 0.9\n",
        "        gamma2 *= 0.9\n",
        "        gamma3 *= 0.9\n",
        "\n",
        "        if it % 10 == 0:\n",
        "          print(it, \"\\ \", n_iter)         \n",
        "          print(\"compute loss...\")\n",
        "          print(compute_loss(mat, mu, bu, bi, Rk_iu, wij, Nk_iu, cij, baseline_bu, baseline_bi, qi, pu, N_u, yj, l_reg6=l_reg6, l_reg7=l_reg7, l_reg8=l_reg8))\n",
        "\n",
        "    return bu, bi, qi, pu, yj, wij, cij\n",
        "#################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mat_file = path+\"/T.mat\"\n",
        "    mat = io.loadmat(mat_file)['X']\n",
        "    integrated_model(mat, mat_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1542, 111)\n",
            "Pre-processing done.\n",
            "Train...\n",
            "0 \\  200\n",
            "compute loss...\n",
            "(array([21551.74171177]), array([36453.82939282]))\n",
            "10 \\  200\n",
            "compute loss...\n",
            "(array([9301.44185443]), array([22908.66935646]))\n",
            "20 \\  200\n",
            "compute loss...\n",
            "(array([8378.37930334]), array([21692.72359891]))\n",
            "30 \\  200\n",
            "compute loss...\n",
            "(array([8118.55591807]), array([21339.84663438]))\n",
            "40 \\  200\n",
            "compute loss...\n",
            "(array([8032.01655141]), array([21221.8330623]))\n",
            "50 \\  200\n",
            "compute loss...\n",
            "(array([8002.2944576]), array([21181.25054177]))\n",
            "60 \\  200\n",
            "compute loss...\n",
            "(array([7991.98630232]), array([21167.1692859]))\n",
            "70 \\  200\n",
            "compute loss...\n",
            "(array([7988.39882551]), array([21162.26786891]))\n",
            "80 \\  200\n",
            "compute loss...\n",
            "(array([7987.14877246]), array([21160.55987462]))\n",
            "90 \\  200\n",
            "compute loss...\n",
            "(array([7986.713006]), array([21159.9644584]))\n",
            "100 \\  200\n",
            "compute loss...\n",
            "(array([7986.56107581]), array([21159.75686476]))\n",
            "110 \\  200\n",
            "compute loss...\n",
            "(array([7986.5081025]), array([21159.68448317]))\n",
            "120 \\  200\n",
            "compute loss...\n",
            "(array([7986.48963204]), array([21159.65924549]))\n",
            "130 \\  200\n",
            "compute loss...\n",
            "(array([7986.4831918]), array([21159.65044568]))\n",
            "140 \\  200\n",
            "compute loss...\n",
            "(array([7986.48094624]), array([21159.64737739]))\n",
            "150 \\  200\n",
            "compute loss...\n",
            "(array([7986.48016325]), array([21159.64630754]))\n",
            "160 \\  200\n",
            "compute loss...\n",
            "(array([7986.47989025]), array([21159.6459345]))\n",
            "170 \\  200\n",
            "compute loss...\n",
            "(array([7986.47979505]), array([21159.64580444]))\n",
            "180 \\  200\n",
            "compute loss...\n",
            "(array([7986.47976186]), array([21159.64575908]))\n",
            "190 \\  200\n",
            "compute loss...\n",
            "(array([7986.47975029]), array([21159.64574327]))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}