{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gat.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuMuZQH3ZUyZ65FL29sahT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanbitlee/summer_2021/blob/main/gat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU4foXjRvTBy",
        "outputId": "ca222189-5b90-4800-9bfb-bf5ee2e42050"
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.5.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVlKs5mSWLwL"
      },
      "source": [
        "from dgl.nn.pytorch import GATConv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB2CkYO9WO_R"
      },
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.g = g\n",
        "        # F dimension's feature space passes through fc-layer and embed on to F' dimension\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        # return attention coefficient\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        # apply Leaky ReLU on to the value from attencion fc \n",
        "        # src stands for source vertex and dst stands for destination vertex\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        # dgl provides an api called update_all which parallely applies function to all nodes\n",
        "        # this function sends tensor to use that api\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        # each node has many neighbors so it has multiple attention coefficients\n",
        "        # use softmax function and sum up element wise\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        # equation (4)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, h):\n",
        "        # embed features using fc layer\n",
        "        z = self.fc(h)\n",
        "        # save embedded vector onto graph\n",
        "        self.g.ndata['z'] = z\n",
        "        # apply_Edges api onto all endges and caclualte attention coefficient between i and j\n",
        "        self.g.apply_edges(self.edge_attention)\n",
        "        # send z and e as tensors on to reduce_func and get new h'\n",
        "        self.g.update_all(self.message_func, self.reduce_func)\n",
        "        return self.g.ndata.pop('h')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-d1wLGfXNUp"
      },
      "source": [
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
        "        super(MultiHeadGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
        "        self.merge = merge\n",
        "\n",
        "    def forward(self, h):\n",
        "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
        "        if self.merge == 'cat':\n",
        "            # concat on the output feature dimension (dim=1)\n",
        "            return torch.cat(head_outs, dim=1)\n",
        "        else:\n",
        "            # merge using average\n",
        "            return torch.mean(torch.stack(head_outs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AduxGtL0W441"
      },
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
        "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
        "        # multiple head outputs are concatenated together. Also, only\n",
        "        # one attention head in the output layer.\n",
        "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h = self.layer1(h)\n",
        "        h = F.elu(h)\n",
        "        h = self.layer2(h)\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyId8fbzYHVG"
      },
      "source": [
        "from dgl import DGLGraph\n",
        "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset \n",
        "import networkx as nx\n",
        "\n",
        "def load_data():\n",
        "    data = CiteseerGraphDataset()\n",
        "    features = torch.FloatTensor(data.features)\n",
        "    labels = torch.LongTensor(data.labels)\n",
        "    train_mask = torch.BoolTensor(data.train_mask)\n",
        "    test_mask = torch.BoolTensor(data.test_mask)\n",
        "    g = DGLGraph(data.graph)\n",
        "    return g, features, labels, train_mask, test_mask"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jwZ8kjlfvPF"
      },
      "source": [
        "def accuracy(logits, labels):\n",
        "    _, indices = torch.max(logits, dim=1)\n",
        "    correct = torch.sum(indices == labels)\n",
        "    return correct.item() * 1.0 / len(labels)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVMzJFeSfy5V"
      },
      "source": [
        "def evaluate(model, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        return accuracy(logits, labels)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ANPWY8kXAEI",
        "outputId": "6c420e6e-1c18-4ccc-fe35-cc293edcc792"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# create the model, 2 heads, each head has hidden size 8\n",
        "\n",
        "g, features, labels, train_mask, test_mask = load_data()\n",
        "\n",
        "net = GAT(g,\n",
        "          in_dim=features.size()[1],\n",
        "          hidden_dim=8,\n",
        "          out_dim=7,\n",
        "          num_heads=2)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# main loop\n",
        "dur = []\n",
        "for epoch in range(100):\n",
        "    if epoch >= 3:\n",
        "        t0 = time.time()\n",
        "\n",
        "    logits = net(features)\n",
        "    logp = F.log_softmax(logits, 1)\n",
        "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch >= 3:\n",
        "        dur.append(time.time() - t0)\n",
        "\n",
        "    acc = accuracy(logits[test_mask], labels[test_mask])\n",
        "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
        "            epoch, loss.item(), acc, np.mean(dur)))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.feat will be deprecated, please use g.ndata['feat'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.label will be deprecated, please use g.ndata['label'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.train_mask will be deprecated, please use g.ndata['train_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.test_mask will be deprecated, please use g.ndata['test_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Loss 1.9455 | Test Acc 0.1600 | Time(s) nan\n",
            "Epoch 00001 | Loss 1.9430 | Test Acc 0.2140 | Time(s) nan\n",
            "Epoch 00002 | Loss 1.9404 | Test Acc 0.2810 | Time(s) nan\n",
            "Epoch 00003 | Loss 1.9379 | Test Acc 0.3300 | Time(s) 0.1509\n",
            "Epoch 00004 | Loss 1.9353 | Test Acc 0.3760 | Time(s) 0.1563\n",
            "Epoch 00005 | Loss 1.9327 | Test Acc 0.4160 | Time(s) 0.1524\n",
            "Epoch 00006 | Loss 1.9301 | Test Acc 0.4390 | Time(s) 0.1522\n",
            "Epoch 00007 | Loss 1.9275 | Test Acc 0.4680 | Time(s) 0.1512\n",
            "Epoch 00008 | Loss 1.9249 | Test Acc 0.4920 | Time(s) 0.1519\n",
            "Epoch 00009 | Loss 1.9223 | Test Acc 0.5090 | Time(s) 0.1518\n",
            "Epoch 00010 | Loss 1.9197 | Test Acc 0.5210 | Time(s) 0.1528\n",
            "Epoch 00011 | Loss 1.9171 | Test Acc 0.5310 | Time(s) 0.1522\n",
            "Epoch 00012 | Loss 1.9144 | Test Acc 0.5340 | Time(s) 0.1522\n",
            "Epoch 00013 | Loss 1.9118 | Test Acc 0.5370 | Time(s) 0.1521\n",
            "Epoch 00014 | Loss 1.9091 | Test Acc 0.5440 | Time(s) 0.1531\n",
            "Epoch 00015 | Loss 1.9065 | Test Acc 0.5440 | Time(s) 0.1530\n",
            "Epoch 00016 | Loss 1.9038 | Test Acc 0.5460 | Time(s) 0.1528\n",
            "Epoch 00017 | Loss 1.9011 | Test Acc 0.5500 | Time(s) 0.1533\n",
            "Epoch 00018 | Loss 1.8983 | Test Acc 0.5560 | Time(s) 0.1529\n",
            "Epoch 00019 | Loss 1.8956 | Test Acc 0.5590 | Time(s) 0.1525\n",
            "Epoch 00020 | Loss 1.8929 | Test Acc 0.5590 | Time(s) 0.1526\n",
            "Epoch 00021 | Loss 1.8901 | Test Acc 0.5610 | Time(s) 0.1531\n",
            "Epoch 00022 | Loss 1.8873 | Test Acc 0.5660 | Time(s) 0.1530\n",
            "Epoch 00023 | Loss 1.8846 | Test Acc 0.5700 | Time(s) 0.1530\n",
            "Epoch 00024 | Loss 1.8817 | Test Acc 0.5710 | Time(s) 0.1534\n",
            "Epoch 00025 | Loss 1.8789 | Test Acc 0.5710 | Time(s) 0.1531\n",
            "Epoch 00026 | Loss 1.8761 | Test Acc 0.5720 | Time(s) 0.1530\n",
            "Epoch 00027 | Loss 1.8732 | Test Acc 0.5790 | Time(s) 0.1531\n",
            "Epoch 00028 | Loss 1.8704 | Test Acc 0.5800 | Time(s) 0.1528\n",
            "Epoch 00029 | Loss 1.8675 | Test Acc 0.5840 | Time(s) 0.1526\n",
            "Epoch 00030 | Loss 1.8646 | Test Acc 0.5840 | Time(s) 0.1528\n",
            "Epoch 00031 | Loss 1.8617 | Test Acc 0.5890 | Time(s) 0.1525\n",
            "Epoch 00032 | Loss 1.8587 | Test Acc 0.5890 | Time(s) 0.1526\n",
            "Epoch 00033 | Loss 1.8558 | Test Acc 0.5910 | Time(s) 0.1527\n",
            "Epoch 00034 | Loss 1.8528 | Test Acc 0.5910 | Time(s) 0.1528\n",
            "Epoch 00035 | Loss 1.8498 | Test Acc 0.5900 | Time(s) 0.1526\n",
            "Epoch 00036 | Loss 1.8468 | Test Acc 0.5920 | Time(s) 0.1527\n",
            "Epoch 00037 | Loss 1.8438 | Test Acc 0.5940 | Time(s) 0.1526\n",
            "Epoch 00038 | Loss 1.8408 | Test Acc 0.5930 | Time(s) 0.1525\n",
            "Epoch 00039 | Loss 1.8377 | Test Acc 0.5930 | Time(s) 0.1524\n",
            "Epoch 00040 | Loss 1.8346 | Test Acc 0.5950 | Time(s) 0.1524\n",
            "Epoch 00041 | Loss 1.8315 | Test Acc 0.5970 | Time(s) 0.1528\n",
            "Epoch 00042 | Loss 1.8284 | Test Acc 0.5970 | Time(s) 0.1530\n",
            "Epoch 00043 | Loss 1.8253 | Test Acc 0.5980 | Time(s) 0.1532\n",
            "Epoch 00044 | Loss 1.8221 | Test Acc 0.5960 | Time(s) 0.1534\n",
            "Epoch 00045 | Loss 1.8190 | Test Acc 0.5970 | Time(s) 0.1534\n",
            "Epoch 00046 | Loss 1.8158 | Test Acc 0.5960 | Time(s) 0.1534\n",
            "Epoch 00047 | Loss 1.8126 | Test Acc 0.5960 | Time(s) 0.1533\n",
            "Epoch 00048 | Loss 1.8093 | Test Acc 0.5970 | Time(s) 0.1532\n",
            "Epoch 00049 | Loss 1.8061 | Test Acc 0.5970 | Time(s) 0.1531\n",
            "Epoch 00050 | Loss 1.8028 | Test Acc 0.5970 | Time(s) 0.1533\n",
            "Epoch 00051 | Loss 1.7996 | Test Acc 0.5990 | Time(s) 0.1532\n",
            "Epoch 00052 | Loss 1.7962 | Test Acc 0.5990 | Time(s) 0.1533\n",
            "Epoch 00053 | Loss 1.7929 | Test Acc 0.5980 | Time(s) 0.1531\n",
            "Epoch 00054 | Loss 1.7896 | Test Acc 0.6010 | Time(s) 0.1532\n",
            "Epoch 00055 | Loss 1.7862 | Test Acc 0.6010 | Time(s) 0.1531\n",
            "Epoch 00056 | Loss 1.7828 | Test Acc 0.6030 | Time(s) 0.1531\n",
            "Epoch 00057 | Loss 1.7794 | Test Acc 0.6030 | Time(s) 0.1530\n",
            "Epoch 00058 | Loss 1.7760 | Test Acc 0.6040 | Time(s) 0.1530\n",
            "Epoch 00059 | Loss 1.7726 | Test Acc 0.6050 | Time(s) 0.1529\n",
            "Epoch 00060 | Loss 1.7691 | Test Acc 0.6070 | Time(s) 0.1531\n",
            "Epoch 00061 | Loss 1.7656 | Test Acc 0.6080 | Time(s) 0.1532\n",
            "Epoch 00062 | Loss 1.7621 | Test Acc 0.6090 | Time(s) 0.1531\n",
            "Epoch 00063 | Loss 1.7586 | Test Acc 0.6120 | Time(s) 0.1532\n",
            "Epoch 00064 | Loss 1.7551 | Test Acc 0.6120 | Time(s) 0.1532\n",
            "Epoch 00065 | Loss 1.7515 | Test Acc 0.6120 | Time(s) 0.1532\n",
            "Epoch 00066 | Loss 1.7479 | Test Acc 0.6130 | Time(s) 0.1534\n",
            "Epoch 00067 | Loss 1.7443 | Test Acc 0.6130 | Time(s) 0.1533\n",
            "Epoch 00068 | Loss 1.7407 | Test Acc 0.6130 | Time(s) 0.1533\n",
            "Epoch 00069 | Loss 1.7371 | Test Acc 0.6120 | Time(s) 0.1532\n",
            "Epoch 00070 | Loss 1.7334 | Test Acc 0.6130 | Time(s) 0.1532\n",
            "Epoch 00071 | Loss 1.7297 | Test Acc 0.6150 | Time(s) 0.1532\n",
            "Epoch 00072 | Loss 1.7260 | Test Acc 0.6170 | Time(s) 0.1532\n",
            "Epoch 00073 | Loss 1.7223 | Test Acc 0.6160 | Time(s) 0.1531\n",
            "Epoch 00074 | Loss 1.7186 | Test Acc 0.6180 | Time(s) 0.1531\n",
            "Epoch 00075 | Loss 1.7148 | Test Acc 0.6190 | Time(s) 0.1531\n",
            "Epoch 00076 | Loss 1.7111 | Test Acc 0.6200 | Time(s) 0.1532\n",
            "Epoch 00077 | Loss 1.7073 | Test Acc 0.6230 | Time(s) 0.1531\n",
            "Epoch 00078 | Loss 1.7035 | Test Acc 0.6240 | Time(s) 0.1530\n",
            "Epoch 00079 | Loss 1.6996 | Test Acc 0.6240 | Time(s) 0.1530\n",
            "Epoch 00080 | Loss 1.6958 | Test Acc 0.6250 | Time(s) 0.1529\n",
            "Epoch 00081 | Loss 1.6919 | Test Acc 0.6250 | Time(s) 0.1529\n",
            "Epoch 00082 | Loss 1.6880 | Test Acc 0.6260 | Time(s) 0.1531\n",
            "Epoch 00083 | Loss 1.6841 | Test Acc 0.6270 | Time(s) 0.1531\n",
            "Epoch 00084 | Loss 1.6802 | Test Acc 0.6270 | Time(s) 0.1532\n",
            "Epoch 00085 | Loss 1.6762 | Test Acc 0.6260 | Time(s) 0.1532\n",
            "Epoch 00086 | Loss 1.6723 | Test Acc 0.6270 | Time(s) 0.1531\n",
            "Epoch 00087 | Loss 1.6683 | Test Acc 0.6280 | Time(s) 0.1530\n",
            "Epoch 00088 | Loss 1.6643 | Test Acc 0.6290 | Time(s) 0.1530\n",
            "Epoch 00089 | Loss 1.6603 | Test Acc 0.6290 | Time(s) 0.1530\n",
            "Epoch 00090 | Loss 1.6562 | Test Acc 0.6300 | Time(s) 0.1530\n",
            "Epoch 00091 | Loss 1.6522 | Test Acc 0.6290 | Time(s) 0.1530\n",
            "Epoch 00092 | Loss 1.6481 | Test Acc 0.6290 | Time(s) 0.1530\n",
            "Epoch 00093 | Loss 1.6440 | Test Acc 0.6300 | Time(s) 0.1529\n",
            "Epoch 00094 | Loss 1.6399 | Test Acc 0.6290 | Time(s) 0.1530\n",
            "Epoch 00095 | Loss 1.6358 | Test Acc 0.6290 | Time(s) 0.1529\n",
            "Epoch 00096 | Loss 1.6316 | Test Acc 0.6310 | Time(s) 0.1531\n",
            "Epoch 00097 | Loss 1.6275 | Test Acc 0.6320 | Time(s) 0.1530\n",
            "Epoch 00098 | Loss 1.6233 | Test Acc 0.6320 | Time(s) 0.1531\n",
            "Epoch 00099 | Loss 1.6191 | Test Acc 0.6330 | Time(s) 0.1530\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}